import torch
import torch.nn.functional as F
from typing import List, Dict
from transformers import AutoModelForSequenceClassification, AutoTokenizer

class SentimentModel:
    def __init__(self):
        # KlasÃ¶r yolu indir.py ile aynÄ± olmalÄ±
        self.model_path = "src/model_files"
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def _load_model(self):
        """Lazy loading: Model sadece ilk istek geldiÄŸinde belleÄŸe yÃ¼klenir."""
        print(f"ğŸ”„ Model yÃ¼kleniyor (Cihaz: {self.device})...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            print("âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            raise e

    def predict_batch(self, texts: List[str]) -> List[Dict]:
        """
        Multilingual Batch Prediction
        Model Ã‡Ä±ktÄ±sÄ±: 1-5 YÄ±ldÄ±z (0-4 arasÄ± indeks)
        Mapping: 1-2 YÄ±ldÄ±z -> Negatif, 3 YÄ±ldÄ±z -> NÃ¶tr, 4-5 YÄ±ldÄ±z -> Pozitif
        """
        if self.model is None or self.tokenizer is None:
            self._load_model()

        # 1. Tokenization (Batch iÃ§in padding ÅŸart)
        encoded = self.tokenizer(
            texts,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=512
        )

        # 2. Veriyi GPU/CPU'ya taÅŸÄ± (GÃ¼venli yÃ¶ntem)
        inputs = {k: v.to(self.device) for k, v in encoded.items()}

        # 3. Inference (Ã‡Ä±karÄ±m)
        with torch.inference_mode():  # no_grad'dan daha hÄ±zlÄ±dÄ±r
            outputs = self.model(**inputs)

        # 4. OlasÄ±lÄ±klarÄ± Hesapla
        probs = F.softmax(outputs.logits, dim=1)
        confidence_scores, predicted_classes = torch.max(probs, dim=1)

        results = []
        for i in range(len(texts)):
            # Model Ã§Ä±ktÄ±sÄ± 0-4 arasÄ±dÄ±r (0=1 yÄ±ldÄ±z, 4=5 yÄ±ldÄ±z)
            star_rating = predicted_classes[i].item() + 1
            score = round(float(confidence_scores[i].item()), 4)

            # --- YILDIZ MAPPING MANTIÄI ---
            if star_rating <= 2:
                sentiment = "Negatif"
            elif star_rating == 3:
                sentiment = "NÃ¶tr"
            else:  # 4 ve 5 yÄ±ldÄ±z
                sentiment = "Pozitif"

            results.append({
                "sentiment": sentiment,
                "confidence": score,
                # Debug iÃ§in yÄ±ldÄ±z bilgisini de loglarda gÃ¶rmek istersen:
                # "stars": star_rating 
            })

        return results

    def predict(self, text: str) -> Dict:
        """Tekli tahmin iÃ§in wrapper (Eski kodlarla uyumluluk iÃ§in)."""
        results = self.predict_batch([text])
        return results[0] if results else {"sentiment": "NÃ¶tr", "confidence": 0.0}